#!/bin/bash

__VERSION__=0.7.0

# Example cluster mode invocation
# bsub -q long \
#    -J idas-pyhail-spark-test-5 \
#    -u idas@wustl.edu -N \
#    -n 32 \
#    -R "span[ptile=8]" \
#    -oo /gscmnt/gc2802/halllab/idas/laboratory/hail-play/idas-test-3/%J-lsf.log \
#    /gscmnt/gc2802/halllab/idas/laboratory/hail-play/pyhail-spark \
#    cluster pyhail-test.py

# http://stackoverflow.com/questions/9893667/is-there-a-way-to-write-a-bash-function-which-aborts-the-whole-execution-no-mat
trap "exit 1" TERM
export TOP_PID=$$

ROOT=/gscmnt/gc2802/halllab/idas

# the JVM
export JAVA_HOME=/gapp/x64linux/opt/java/jdk/jdk1.8.0_60
JAVA_BIN=${JAVA_HOME}/bin

# needed because of the c/c++ code embedded in hail
GCC_BIN=/opt/gcc-4.8.4/bin
GCC_LIBS=/opt/gcc-4.8.4/lib64
OPENBLAS_LIBS=/gscmnt/gc2802/halllab/idas/software/local/lib
CMAKE_BIN=/gscuser/idas/software/cmake/bin

# setup the needed environment variables
export PATH=${CMAKE_BIN}:${GCC_BIN}:${JAVA_BIN}:${PATH}
export LD_LIBRARY_PATH=${GCC_LIBS}:${OPENBLAS_LIBS}:${LD_LIBRARY_PATH}

# SPARK software location and setup
export SPARK_HOME=${ROOT}/software/local/spark-2.0.2-bin-hadoop2.7

: "${APACHE_SPARK_MASTER_PORT:=7077}"           # can be overridden in environment
export SPARK_MASTER_PORT=${APACHE_SPARK_MASTER_PORT}

: "${APACHE_SPARK_MASTER_WEBUI_PORT:=8080}"     # can be overridden in environment
export SPARK_MASTER_WEBUI_PORT=${APACHE_SPARK_MASTER_WEBUI_PORT}

: "${APACHE_SPARK_LOG_DIR:=$HOME/.spark/logs}"  # can be overridden in environment
export SPARK_LOG_DIR=${APACHE_SPARK_LOG_DIR}

# Seconds to wait for all the spart workers/executors to fully start up
# and connect to the master node.  Sometimes NFS can be very, very sluggish
: "${SPARK_MASTER_SETUP_CONNECTION_WAIT_TIME:=30}"  # can be overridden in environment
: "${SPARK_WORKER_SETUP_CONNECTION_WAIT_TIME:=120}" # can be overridden in environment

# HAIL software locations
export HAIL_HOME=${ROOT}/software/downloads/github/hail
export LOCAL_HAIL=${HAIL_HOME}/build/install/hail/bin/hail
export SPARK_HAIL_JAR=${HAIL_HOME}/build/libs/hail-all-spark.jar

# the PYTHON that should be used
export PYSPARK_PYTHON=/gscuser/idas/.pyenv/versions/2.7.10/bin/python

# set -o xtrace
# /bin/bash ${LSF_SPARK_WRAPPER} \
#     --class org.broadinstitute.hail.driver.Main \
#     ${SPARK_HAIL_JAR} \
#     $@
# set +o xtrace

function log {
    local timestamp=$(date +"%Y-%m-%d %T")
    echo "==== [ ${timestamp} ] $@ ====" >&2
}

function note {
    local timestamp=$(date +"%Y-%m-%d %T")
    echo "---> [ ${timestamp} ] $@" >&2
}

function subnote {
    local msg=$1
    local timestamp=$(date +"%Y-%m-%d %T")
    echo "--->> [ ${timestamp} ] $@" >&2
}

function die {
    local timestamp=$(date +"%Y-%m-%d %T")
    echo "[ ${timestamp} ] ERROR: $@" >&2
    kill -s TERM ${TOP_PID}
}

function check_remote_file_exists {
    local host=$1
    local file_path=$2
    local cmd=
    local check=$(ssh -tq -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -l ${USER} ${host} [[ -f ${file_path} ]] && echo "yes" || echo "no";)
    if [ "$check" == "no" ]; then
        die "Did not find ${file_path} on '${host}' !"
    fi
}

function exec_ssh_cmd {
    local host=$1
    local cmd=$2
    local debug=$3

    note "SSH: ${host} | CMD: ${cmd}"

    if [ -n "$debug" ]; then
        set -o xtrace
    fi

    ssh -tq \
        -o StrictHostKeyChecking=no \
        -o UserKnownHostsFile=/dev/null \
        -l ${USER} \
        ${host} \
        "${cmd}"

    set +o xtrace
}

function exec_ssh_cmd_with_rv {
    local host=$1
    local cmd=$2
    local debug=$3
    local out=

    note "SSH: ${host} | CMD: ${cmd}"

    if [ -n "$debug" ]; then
        set -o xtrace
    fi

    out=$(ssh -tq \
        -o StrictHostKeyChecking=no \
        -o UserKnownHostsFile=/dev/null \
        -l ${USER} \
        ${host} \
        "${cmd}")

    set +o xtrace

    echo ${out}
}

function ensure_dir {
    local directory=$1
    local msg=$2
	if [ ! -d "${directory}" ]; then
		note "${msg}: ${directory}"
		mkdir -p "${directory}"
	fi
}

function ensure_spark_log_dir {
    ensure_dir ${SPARK_LOG_DIR} "Creating spark log directory"
}

function ensure_spark_worker_log_dir {
    local directory=$1
    ensure_dir ${directory} "Creating spark cluster worker log directory"
}

function check_in_lsf_cluster {
    err="LSB_MCPU_HOSTS environment variable not found! "
    err+="This is not a multi-node LSF job!"
    : "${LSB_MCPU_HOSTS:?${err}}"
}

function parse_lsf_cpu_hosts {
    local -a cpu_hosts=(${LSB_MCPU_HOSTS})
    declare -A host_cpu_dictionary
    local j=
    local host=
    local cpus=
    for (( i = 0; i < ${#cpu_hosts[@]}; i += 2 )); do
        j=$(( $i + 1 ))
        host=${cpu_hosts[$i]}
        cpus=${cpu_hosts[$j]}
        host_cpu_dictionary[${host}]=${cpus}
    done
    declare -p host_cpu_dictionary | sed -e 's/^declare -A host_cpu_dictionary=//'
}

function start_spark_master {
    ${SPARK_HOME}/sbin/start-master.sh > /dev/null
    log "Successfully launched spark master node"
    note "Sleeping for ${SPARK_MASTER_SETUP_CONNECTION_WAIT_TIME} seconds to allow spark master to setup"
    note "(Setup Spark Master) Start"
    sleep ${SPARK_MASTER_SETUP_CONNECTION_WAIT_TIME}
    note "(Setup Spark Master) Finish"
}

function ensure_spark_worker_dir {
    local host=$1
    local cmd="mkdir -p ${SPARK_WORKER_DIR}"
    subnote "Worker: ${host} -- ensure spark worker dir: ${SPARK_WORKER_DIR}"
    exec_ssh_cmd ${host} "${cmd}"
}

function create_spark_worker_properties_file {
    local host=$1
    local properties_file=$2
    local work_dir=$3

    read -r -d '' properties_conf <<-EOF
		spark.local.dir        ${work_dir}
		spark.network.timeout  300
		EOF
    local cmd="echo '${properties_conf}' > ${properties_file}"
    exec_ssh_cmd ${host} "${cmd}"
    check_remote_file_exists ${host} ${properties_file}
}

function remove_spark_worker_dir {
    local host=$1
    subnote "Worker: ${host} -- removing spark worker dir: ${SPARK_WORKER_DIR}"
    local cmd="rm -rf ${SPARK_WORKER_DIR}"
    exec_ssh_cmd ${host} "${cmd}"
}

function start_spark_workers {
    local -A spark_workers
    local i=
    local -i count=0
    eval "local -A nodes=${1#*=}"
    local worker_log_dir=${SPARK_LOG_DIR}/worker-logs
    local properties_file=${SPARK_WORKER_DIR}/spark-worker-defaults.conf

    ensure_spark_worker_log_dir ${worker_log_dir}

    for i in "${!nodes[@]}"; do
        count=$(( $count + 1 ))
        local host=$i
        local cpus=${nodes[$host]}
        note "Starting Spark Worker ${count} -- host : ${host} | cpus : ${cpus}"
        ensure_spark_worker_dir ${host}
        note "Creating Spark Worker property/conf file: ${properties_file}"
        create_spark_worker_properties_file ${host} ${properties_file} ${SPARK_WORKER_DIR}

        local cmd="nohup ${SPARK_HOME}/bin/spark-class org.apache.spark.deploy.worker.Worker "
        cmd+="spark://${SPARK_MASTER}:${SPARK_MASTER_PORT} "
        cmd+="-c ${cpus} "
        cmd+="--work-dir ${SPARK_WORKER_DIR} "
        cmd+="--properties-file ${properties_file} "
        cmd+="< /dev/null "
        cmd+="> ${worker_log_dir}/spark-worker-${host}.log "
        cmd+="2>&1 & echo \$!"

        local pid=$(exec_ssh_cmd_with_rv ${host} "${cmd}")

        local worker_msg="Spark Worker ${count} -- host : "
        worker_msg+="${host} | cpus : "
        worker_msg+="${cpus} | pid : ${pid}"
        subnote ${worker_msg}

        spark_workers[${host}]=${pid}
    done

    # Wait a few moments for all the workers to fully start up
    # and connect to the master node
    local msg="Sleeping for ${SPARK_WORKER_SETUP_CONNECTION_WAIT_TIME} seconds "
    msg+="to allow spark worker/executor connections to setup"
    note ${msg}
    note "(Setup Spark Worker Connections) Start"
    sleep ${SPARK_WORKER_SETUP_CONNECTION_WAIT_TIME}
    note "(Setup Spark Worker Connections) Finish"

    declare -p spark_workers | sed -e 's/^declare -A spark_workers=//'
}

function stop_spark_workers {
    local -A spark_workers
    eval "local -A spark_workers=${1#*=}"

    for i in "${!spark_workers[@]}"; do
        local host=$i
        local pid=${spark_workers[$host]}
        note "Stopping Spark Worker -- host : ${host} | pid : ${pid}"
        local cmd="kill ${pid}"
        exec_ssh_cmd ${host} "${cmd}"
        remove_spark_worker_dir ${host}
    done
}

function stop_spark_master {
    $SPARK_HOME/sbin/stop-master.sh > /dev/null
}

function run_pyhail_cmd {
    local spark_python_libs=${SPARK_HOME}/python/lib
    local hail_python_libs=${HAIL_HOME}/python/lib

    local py4j=${spark_python_libs}/py4j-0.10.3-src.zip
    local pyspark=${spark_python_libs}/pyspark.zip
    local pyhail=${hail_python_libs}/pyhail.zip

    note "(Hail Command) Start"
    set -o xtrace
    ${SPARK_HOME}/bin/spark-submit \
        --jars ${SPARK_HAIL_JAR} \
        --py-files ${py4j},${pyspark},${pyhail} \
        --master spark://$SPARK_MASTER:$SPARK_MASTER_PORT \
        $@
    set +o xtrace
    note "(Hail Command) Finish"
}

function cluster {
    check_in_lsf_cluster

    export SPARK_LOG_DIR=${SPARK_LOG_DIR}/cluster/${LSB_JOBID}
    ensure_spark_log_dir

    # setup SPARK CLUSTER environment variables
    export SPARK_MASTER=$(hostname)
    export SPARK_PUBLIC_DNS=$(/sbin/ifconfig | grep -A 1 eth0 | awk '/inet addr/{print substr($2,6)}')

    # set spark worker dir (can be overridden in environment)
    : "${SPARK_WORKER_DIR:=/tmp/$LSB_JOBID.tmpdir}"

    local -A lsf_hosts
    local -A spark_workers

    log "SPARK_HOME: ${SPARK_HOME}"
    log "SPARK_MASTER: ${SPARK_MASTER}"
    log "SPARK_PUBLIC_DNS: ${SPARK_PUBLIC_DNS}"
    log "SPARK_MASTER_WEBUI: http://${SPARK_PUBLIC_DNS}:${SPARK_MASTER_WEBUI_PORT}"
    log "LSB_MCPU_HOSTS : ${LSB_MCPU_HOSTS}"

    log "Gathering HOST and CPU info"
    cpu_hosts_string=$(parse_lsf_cpu_hosts)
    eval "local -A lsf_hosts="${cpu_hosts_string}

    log "Starting spark master node: spark://${SPARK_MASTER}:${SPARK_MASTER_PORT}"
    start_spark_master

    log "Starting spark worker nodes"
    spark_workers_string=$(start_spark_workers "$(declare -p lsf_hosts)")
    eval "local -A spark_workers="${spark_workers_string}

    log "Exec pyhail command"
    run_pyhail_cmd $@

    log "Stop Spark Master"
    stop_spark_master

    log "Stop Spark Workers"
    stop_spark_workers "$(declare -p spark_workers)"
}

function local_pyspark {
    export SPARK_LOG_DIR="${SPARK_LOG_DIR}/local/${LSB_JOBID}"
    ensure_spark_log_dir
    local py4j=${SPARK_HOME}/python/lib/py4j-0.10.3-src.zip
    local spark_python_libs=${SPARK_HOME}/python
    local hail_python_libs=${HAIL_HOME}/python
    export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.3-src.zip:$HAIL_HOME/python
    export SPARK_CLASSPATH=${SPARK_HAIL_JAR}
    ${PYSPARK_PYTHON}
}

function help_msg {
    local msg=
    read -r -d '' msg <<-EOF
	    Usage: pyhail-spark COMMAND [ARGS]...
	    
	      A collection of pyhail wrappers for using at the MGI
	    
	    Commands:
	      help         Show this message and exit
	      version      Show version number and exit
	      local        Locally run spark and get a python shell with acceses to pyhail
	      cluster      create a apache spark cluster and run a python script supplied 
	                   as an argument (must be used via LSF/bsub)
	     
	    Environment Variables:
	     
	    Use these environment variables to adjust how pyhail-spark operates:
	     
	      APACHE_SPARK_LOG_DIR                       Directory where Spark places its logs
	                                                 (default: \$HOME/.spark/logs)
	
	      APACHE_SPARK_MASTER_PORT                   The port Spark uses for the master node during cluster
	                                                 operations (default: 7077)
	
	      APACHE_SPARK_MASTER_WEBUI_PORT             The WebUI port Spark uses for the master node during
	                                                 cluster operations (default: 8080)
	
	      SPARK_MASTER_SETUP_CONNECTION_WAIT_TIME    The number of seconds to wait for the spark master node
	                                                 to fully startup during cluster operations (default: 30)
	
	      SPARK_WORKER_SETUP_CONNECTION_WAIT_TIME    The number of seconds to wait for the master node
	                                                 to fully startup during cluster operations (default: 120)
	
	      SPARK_WORKER_DIR                           The workspace directory to use for the spark worker/executor
	                                                 nodes to use during cluster operations
	                                                 (default: /tmp/$LSB_JOBID.tmpdir)
	EOF
    echo "${msg}"
}

function main {
    local exec_type=$1
    shift
    
    case ${exec_type} in 
        help|--help)
            local msg=$(help_msg)
            echo "${msg}"
            ;;
        version)
            echo $__VERSION__
            ;;
        local)
            log "Running pyhail repl in local mode."
            local_pyspark
            ;;
        cluster)
            log "Running pyhail in a standalone spark cluster."
            log "the pyhail args are:" $@
            cluster $@
            ;;
        *)
            local msg=$(help_msg)
            die "${msg}"
            ;;
    esac
}

main $@
